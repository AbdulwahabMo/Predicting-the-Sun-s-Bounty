# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WwwehHYkk3eyZGBKduaneRkXh7FlMSuI
"""

########################################### LSTM MODEL #################################################

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Load the dataset
dataset = pd.read_csv('/content/dataset.csv')

# Last column is the output power
output_power_column = -1

# Extract features and labels
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, output_power_column].values.reshape(-1, 1)  # Reshape for MinMaxScaler

# Normalize the data using Min-Max scaling for both input and output
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

X = scaler_x.fit_transform(X)
y = scaler_y.fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the input data for LSTM (The input shape is (samples, time steps, features))
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1))  # Output layer with one neuron for regression
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the LSTM model
history_lstm = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=2)

# Plot training and validation errors for LSTM model
plt.figure(figsize=(12, 6))
plt.plot(history_lstm.history['loss'], label='LSTM Training Loss')
plt.plot(history_lstm.history['val_loss'], label='LSTM Validation Loss')

plt.title('Training and Validation Errors (LSTM Model Only)')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

# Evaluate the model on the test set
y_pred_lstm = model.predict(X_test)
y_pred_lstm = scaler_y.inverse_transform(y_pred_lstm)  # Inverse transform to get the original scale
y_test_orig_lstm = scaler_y.inverse_transform(y_test)  # Inverse transform to get the original scale

# Calculate R2 score
r2_lstm = r2_score(y_test_orig_lstm, y_pred_lstm)
print(f'R2 Score (LSTM): {r2_lstm}')

# Find the epoch with the minimum validation loss for LSTM
min_val_loss_epoch_lstm = np.argmin(history_lstm.history['val_loss']) + 1  # Add 1 because epochs start from 1

# Plot training and validation errors for LSTM model only with legend, min val loss, and R2 score
plt.figure(figsize=(12, 6))
plt.plot(history_lstm.history['loss'], label='LSTM Training Loss')
plt.plot(history_lstm.history['val_loss'], label='LSTM Validation Loss')
plt.scatter(min_val_loss_epoch_lstm, min(history_lstm.history['val_loss']), color='red', marker='o', label='Min Val Loss (LSTM)')

plt.title('Training and Validation Errors (LSTM Model Only)')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()

# Add text for R2 score
plt.text(0.5, 0.8, f'R2 Score (LSTM): {r2_lstm:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

plt.show()

########################################### GATED RECURRENT UNIT #################################################

from keras.layers import GRU

# Load dataset
dataset = pd.read_csv('/content/dataset.csv')

# Last column is the output power
output_power_column = -1

# Extract features and labels
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, output_power_column].values.reshape(-1, 1)  # Reshape for MinMaxScaler

# Normalize the data using Min-Max scaling for both input and output
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

X = scaler_x.fit_transform(X)
y = scaler_y.fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the input data for GRU (The input shape is (samples, time steps, features))
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the GRU model
model = Sequential()
model.add(GRU(50, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1))  # Output layer with one neuron for regression
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the GRU model
history_gru = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=2)

# Plot training and validation errors for GRU model
plt.figure(figsize=(12, 6))
plt.plot(history_gru.history['loss'], label='GRU Training Loss')
plt.plot(history_gru.history['val_loss'], label='GRU Validation Loss')

plt.title('Training and Validation Errors (GRU Model)')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

# Evaluate the model on the test set
y_pred_gru = model.predict(X_test)
y_pred_gru = scaler_y.inverse_transform(y_pred_gru)  # Inverse transform to get the original scale
y_test_orig_gru = scaler_y.inverse_transform(y_test)  # Inverse transform to get the original scale

# Calculate R2 score
r2_gru = r2_score(y_test_orig_gru, y_pred_gru)
print(f'R2 Score (GRU): {r2_gru}')

# Find the epoch with the minimum validation loss for GRU
min_val_loss_epoch_gru = np.argmin(history_gru.history['val_loss']) + 1  # Add 1 because epochs start from 1

# Plot training and validation errors for GRU model with legend, min val loss, and R2 score
plt.figure(figsize=(12, 6))
plt.plot(history_gru.history['loss'], label='GRU Training Loss')
plt.plot(history_gru.history['val_loss'], label='GRU Validation Loss')
plt.scatter(min_val_loss_epoch_gru, min(history_gru.history['val_loss']), color='red', marker='o', label='Min Val Loss (GRU)')

plt.title('Training and Validation Errors (GRU Model)')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()

# Add text for R2 score
plt.text(0.5, 0.8, f'R2 Score (GRU): {r2_gru:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

plt.show()

########################################### CONV LSTM #################################################

from keras.layers import Conv1D, Flatten
from sklearn.linear_model import Lasso

# Load dataset
dataset = pd.read_csv('/content/dataset.csv')

# Assuming the last column is the output power
output_power_column = -1

# Extract features and labels
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, output_power_column].values.reshape(-1, 1)

# Normalize the data using Min-Max scaling for both input and output
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()
X = scaler_x.fit_transform(X)
y = scaler_y.fit_transform(y)

# Split the dataset into training and testing sets for Conv1D and LSTM
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reshape the input data for Conv1D and LSTM
X_train_cnn_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Build the Conv1D and LSTM model
model_cnn_lstm = Sequential()
model_cnn_lstm.add(Conv1D(filters=50, kernel_size=3, activation='relu', input_shape=(X_train_cnn_lstm.shape[1], 1)))
model_cnn_lstm.add(LSTM(50, activation='relu'))
model_cnn_lstm.add(Dense(1))  # Output layer with one neuron for regression
model_cnn_lstm.compile(optimizer='adam', loss='mean_squared_error')

# Train the Conv1D and LSTM model
history_cnn_lstm = model_cnn_lstm.fit(X_train_cnn_lstm, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn_lstm, y_test), verbose=2)

# Evaluate the model on the test set
y_pred = model_cnn_lstm.predict(X_test)
y_pred = scaler_y.inverse_transform(y_pred)  # Inverse transform to get the original scale
y_test_orig = scaler_y.inverse_transform(y_test)  # Inverse transform to get the original scale

# Calculate R2 score
r2 = r2_score(y_test_orig, y_pred)
print(f'R2 Score: {r2}')

# Plot training and validation errors for Conv1D and LSTM
plt.figure(figsize=(12, 6))
plt.plot(history_cnn_lstm.history['loss'], label='Training Loss (Conv1D-LSTM)')
plt.plot(history_cnn_lstm.history['val_loss'], label='Validation Loss (Conv1D-LSTM)')
# Add R2 score to the plot
plt.text(0.5, 0.5, f'R2 Score: {r2:.2f}', fontsize=12, ha='center', transform=plt.gcf().transFigure)
plt.title('Training & Validation Errors')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Errors (MSE)')
plt.legend()
plt.show()

# Lasso Regression
lasso = Lasso(alpha=0.001)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

# Extract feature importances from Lasso
lasso_coeff = pd.DataFrame({'Feature Importance': lasso.coef_}, index=dataset.columns[:-1])
lasso_coeff.sort_values('Feature Importance', ascending=False)
g = lasso_coeff[lasso_coeff['Feature Importance'] != 0].sort_values('Feature Importance').plot(kind='barh', figsize=(6, 6), cmap='winter')

plt.title('Feature Importance (Lasso)')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.legend()
plt.show()

########################################### TIME2VEC #################################################

from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch

# Load dataset
dataset = pd.read_csv('/content/dataset.csv')

# Splitting the dataset into features and target
features = dataset.drop('generated_power_kw', axis=1)
target = dataset[['generated_power_kw']]

# Normalize the features
feature_scaler = MinMaxScaler()
features = feature_scaler.fit_transform(features)

# Normalize the target
target_scaler = MinMaxScaler()
target = target_scaler.fit_transform(target)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)


class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features).float()
        self.labels = torch.tensor(labels).float()

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = CustomDataset(X_train, y_train)
test_dataset = CustomDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class Time2Vec(nn.Module):
    def __init__(self, input_size, output_size):
        super(Time2Vec, self).__init__()
        self.linear = nn.Linear(input_size, output_size)
        self.periodic = nn.Linear(input_size, output_size)

    def forward(self, x):
        return torch.cat([self.linear(x), torch.sin(self.periodic(x))], -1)

class PredictiveModel(nn.Module):
    def __init__(self, feature_size):
        super(PredictiveModel, self).__init__()
        self.time2vec = Time2Vec(feature_size, 128)
        self.linear1 = nn.Linear(256, 128)  # Time2Vec doubles the feature size
        self.linear2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.time2vec(x)
        x = nn.functional.relu(self.linear1(x))
        return self.linear2(x)

model = PredictiveModel(X_train.shape[1])

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

num_epochs = 50
train_errors = []
val_errors = []
r2_scores = []

for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    for features, labels in train_loader:
        optimizer.zero_grad()
        predictions = model(features)
        loss = criterion(predictions.squeeze(), labels.squeeze())
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    avg_train_loss = total_train_loss / len(train_loader)
    train_errors.append(avg_train_loss)

    model.eval()
    total_val_loss = 0
    predictions_all = []
    labels_all = []
    with torch.no_grad():
        for features, labels in test_loader:
            predictions = model(features)
            loss = criterion(predictions.squeeze(), labels.squeeze())
            total_val_loss += loss.item()

            # Collect predictions and labels for R2 score calculation
            predictions_all.extend(predictions.numpy())
            labels_all.extend(labels.numpy())

    avg_val_loss = total_val_loss / len(test_loader)
    val_errors.append(avg_val_loss)

    # Calculate R2 score
    r2 = r2_score(labels_all, predictions_all)
    r2_scores.append(r2)

    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, R2 Score: {r2:.4f}")

model.eval()
with torch.no_grad():
    total_loss = 0
    predictions_all = []
    labels_all = []
    for features, labels in test_loader:
        predictions = model(features)
        loss = criterion(predictions.squeeze(), labels.squeeze())
        total_loss += loss.item()

        # Collect predictions and labels for R2 score calculation
        predictions_all.extend(predictions.numpy())
        labels_all.extend(labels.numpy())

    # Inverse transform to original scale
    predictions_all = target_scaler.inverse_transform(np.array(predictions_all).reshape(-1, 1))
    labels_all = target_scaler.inverse_transform(np.array(labels_all).reshape(-1, 1))

print(f"Test Loss: {total_loss / len(test_loader)}")

# Calculate R2 score for the entire test set
test_r2 = r2_score(labels_all, predictions_all)
print(f"Test R2 Score: {test_r2}")

# Plotting results
plt.figure(figsize=(10, 6))
plt.scatter(labels_all, predictions_all, alpha=0.6, label='Actual vs Predicted')
plt.plot([labels_all.min(), labels_all.max()], [labels_all.min(), labels_all.max()], 'k--', lw=2, label='Perfect Prediction Line')
plt.xlabel('Actual Generated Power (kW)')
plt.ylabel('Predicted Generated Power (kW)')
plt.title('Actual vs Predicted Generated Power')

# Adding the legend
plt.legend(loc="upper left")

plt.show()

plt.figure(figsize=(10, 6))
plt.plot(train_errors, label='Training MSE')
plt.plot(val_errors, label='Validation MSE')
plt.title('Training and Validation Errors over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

plt.figure

########################################### NBEATS  #################################################

# Load dataset
dataset = pd.read_csv('/content/dataset.csv')

# Splitting the dataset into features and target
features = dataset.drop('generated_power_kw', axis=1)
target = dataset[['generated_power_kw']]

# Normalize the features
feature_scaler = MinMaxScaler()
features = feature_scaler.fit_transform(features)

# Normalize the target
target_scaler = MinMaxScaler()
target = target_scaler.fit_transform(target)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

import torch
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features).float()
        self.labels = torch.tensor(labels).float()

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = CustomDataset(X_train, y_train)
test_dataset = CustomDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class NBeatsBlock(nn.Module):
    def __init__(self, input_size, theta_size, hidden_units):
        super(NBeatsBlock, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_units)
        self.fc2 = nn.Linear(hidden_units, hidden_units)
        self.fc3 = nn.Linear(hidden_units, theta_size)
        self.backcast_fc = nn.Linear(theta_size, input_size)
        self.forecast_fc = nn.Linear(theta_size, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        theta = self.fc3(x)
        backcast = self.backcast_fc(theta)
        forecast = self.forecast_fc(theta)
        return backcast, forecast

class NBeatsModel(nn.Module):
    def __init__(self, input_size, hidden_units, theta_size, num_blocks):
        super(NBeatsModel, self).__init__()
        self.blocks = nn.ModuleList([NBeatsBlock(input_size, theta_size, hidden_units) for _ in range(num_blocks)])

    def forward(self, x):
        forecast = torch.zeros(x.size(0), 1, device=x.device)
        for block in self.blocks:
            backcast, block_forecast = block(x)
            forecast += block_forecast
            x = x - backcast
        return forecast

model = NBeatsModel(input_size=X_train.shape[1], hidden_units=128, theta_size=64, num_blocks=3)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

num_epochs = 50
train_errors = []
val_errors = []

for epoch in range(num_epochs):
    # Training phase
    model.train()
    total_train_loss = 0
    for features, labels in train_loader:
        optimizer.zero_grad()
        predictions = model(features)
        loss = criterion(predictions.squeeze(), labels.squeeze())
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    avg_train_loss = total_train_loss / len(train_loader)
    train_errors.append(avg_train_loss)

    # Validation phase
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for features, labels in test_loader:
            predictions = model(features)
            loss = criterion(predictions.squeeze(), labels.squeeze())
            total_val_loss += loss.item()
    avg_val_loss = total_val_loss / len(test_loader)
    val_errors.append(avg_val_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

model.eval()
predictions = []
actuals = []

with torch.no_grad():
    for features, labels in test_loader:
        preds = model(features).squeeze()
        predictions.extend(preds.numpy())
        actuals.extend(labels.numpy())

# Inverse transform the predictions and actuals
predictions = target_scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
actuals = target_scaler.inverse_transform(np.array(actuals).reshape(-1, 1))

# Calculate R2 score
r2 = r2_score(actuals, predictions)
print(f"R2 Score: {r2:.4f}")

plt.figure(figsize=(10, 6))

# Scatter plot for actual vs predicted values
scatter = plt.scatter(actuals, predictions, alpha=0.6, label='Actual vs Predicted')

# Diagonal line representing perfect predictions
line = plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'k--', lw=2, label='Perfect Prediction Line')

# Adding labels and title
plt.xlabel('Actual Generated Power (kW)')
plt.ylabel('Predicted Generated Power (kW)')
plt.title('Actual vs Predicted Generated Power')

# Adding the legend
plt.legend(loc="upper left")

plt.show()

plt.figure(figsize=(10, 6))
plt.plot(train_errors, label='Training MSE')
plt.plot(val_errors, label='Validation MSE')
plt.title('Training and Validation Errors over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

########################################### VANILLA TRANSFORMER  #################################################
import torch.optim as optim

# Load dataset
dataset = pd.read_csv('/content/dataset.csv')

# Splitting the dataset into features and target
features = dataset.drop('generated_power_kw', axis=1)
target = dataset[['generated_power_kw']]

# Normalize the features
feature_scaler = MinMaxScaler()
features = feature_scaler.fit_transform(features)

# Normalize the target
target_scaler = MinMaxScaler()
target = target_scaler.fit_transform(target)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features).float()
        self.labels = torch.tensor(labels).float()

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

train_dataset = CustomDataset(X_train, y_train)
test_dataset = CustomDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class TransformerModel(nn.Module):
    def __init__(self, input_size, num_heads, num_layers, dim_feedforward, dropout):
        super(TransformerModel, self).__init__()
        self.input_linear = nn.Linear(input_size, dim_feedforward)
        encoder_layers = nn.TransformerEncoderLayer(d_model=dim_feedforward, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
        self.output_linear = nn.Linear(dim_feedforward, 1)

    def forward(self, src):
        src = self.input_linear(src)
        src = src.unsqueeze(1)  # Add batch dimension
        output = self.transformer_encoder(src)
        output = self.output_linear(output.squeeze(1))
        return output

# Define the model
input_size = X_train.shape[1]
model = TransformerModel(input_size=input_size, num_heads=4, num_layers=3, dim_feedforward=128, dropout=0.1)

# Training
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

num_epochs = 50
train_errors = []
val_errors = []

for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    for features, labels in train_loader:
        optimizer.zero_grad()
        predictions = model(features)
        loss = criterion(predictions.squeeze(), labels.squeeze())
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    avg_train_loss = total_train_loss / len(train_loader)
    train_errors.append(avg_train_loss)

    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for features, labels in test_loader:
            predictions = model(features)
            loss = criterion(predictions.squeeze(), labels.squeeze())
            total_val_loss += loss.item()
    avg_val_loss = total_val_loss / len(test_loader)
    val_errors.append(avg_val_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

# Evaluation
model.eval()
predictions = []
actuals = []

with torch.no_grad():
    for features, labels in test_loader:
        preds = model(features).squeeze()
        predictions.extend(preds.numpy())
        actuals.extend(labels.numpy())

predictions = target_scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
actuals = target_scaler.inverse_transform(np.array(actuals).reshape(-1, 1))

# Calculate R2 score
r2 = r2_score(actuals, predictions)
print(f"R2 Score: {r2}")

# Plotting results
plt.figure(figsize=(10, 6))
plt.scatter(actuals, predictions, alpha=0.6, label='Actual vs Predicted')
plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'k--', lw=2, label='Perfect Prediction Line')
plt.xlabel('Actual Generated Power (kW)')
plt.ylabel('Predicted Generated Power (kW)')
plt.title('Actual vs Predicted Generated Power')

# Adding the legend
plt.legend(loc="upper left")

plt.show()

plt.figure(figsize=(10, 6))
plt.plot(train_errors, label='Training MSE')
plt.plot(val_errors, label='Validation MSE')
plt.title('Training and Validation Errors over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

########################################### NON-LINEAR REGRESSION  #################################################

from sklearn.preprocessing import PolynomialFeatures

dataset = pd.read_csv('/content/dataset.csv')

# Last column is the output
output_power_column = -1

# Extract features and labels
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, output_power_column].values.reshape(-1, 1)  # Reshape for MinMaxScaler

# Normalize the data using Min-Max scaling for both input and output
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

X = scaler_x.fit_transform(X)
y = scaler_y.fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to add polynomial features
def add_polynomial_features(X, degree):
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    return poly.fit_transform(X)

# Polynomial Regression Model
degree = 3
X_train_poly = add_polynomial_features(X_train, degree)
X_test_poly = add_polynomial_features(X_test, degree)

model = Sequential()


model.add(Dense(1, input_dim=X_train_poly.shape[1], activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history_poly = model.fit(X_train_poly, y_train, epochs=50, batch_size=32, validation_data=(X_test_poly, y_test), verbose=2)

# Evaluate the model on the test set
predictions_poly = model.predict(X_test_poly)
predictions_poly = scaler_y.inverse_transform(predictions_poly)
y_test_orig_poly = scaler_y.inverse_transform(y_test)
r2_poly = r2_score(y_test_orig_poly, predictions_poly)
print(f'R2 Score (Polynomial Regression, Degree {degree}): {r2_poly}')

# Plot training and validation errors with legend, min val loss, and R2 score
plt.figure(figsize=(12, 6))
plt.plot(history_poly.history['loss'], label='Training Loss')
plt.plot(history_poly.history['val_loss'], label='Validation Loss')

# Find the epoch with the minimum validation loss for Polynomial Regression
min_val_loss_epoch_poly = np.argmin(history_poly.history['val_loss']) + 1  # Add 1 because epochs start from 1
plt.scatter(min_val_loss_epoch_poly, min(history_poly.history['val_loss']), color='red', marker='o', label='Min Val Loss (Poly)')

plt.title(f'Training and Validation Errors (Polynomial Regression, Degree {degree})')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()

# Add text for R2 score
plt.text(0.5, 0.8, f'R2 Score (Poly): {r2_poly:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))

plt.show()